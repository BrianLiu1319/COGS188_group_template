{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceInvaders\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Brian Liu\n",
    "- Valeria Avila\n",
    "- Juan Villalobos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Using OpenAI's Space Invaders gaming environment, we will be training an AI agent to destroy as many invaders as possible by firing a laser cannon before the invaders reach Earth or before they hit the agent. Points are earned each time the agent destroys an invader, therefore, the success of how well the agent works will be determined by the amount of points it scored. Another note when it comes to points is that the invaders in the back rows are worth more points [5, 10, 15, 20, 25, 30] respectfully. OpenAI’s Space invader has a discrete action space of 6 [no action, left, right, fire, fire left, fire right] with an observation space of (0, 255, (210, 160, 3). The invaders are able to shoot at well, which can make the player lose lives. We decided to use Deep Q Learning, with 3 convolutional layers and ReLU as the activator function. The results were measured using the average reward (points obtained by destroying invaders) and time duration (how long the agent was able to play for). We found that the agent was able to play for 17 seconds with an average reward of 200 points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Space Invaders is a classic \"shoot 'em up'' arcade game developed in the 1970s in Japan. Space Invaders is considered one of the most influential video games as it was the first fixed shooter. The way this game works is by having the user control a laser cannon and defeat a wave of aliens before they reach Earth or attack the user. The more aliens that are destroyed, the more points are earned. Where the player gets points by destroying the invaders. The invaders furthest from the player have the highest values, while the closest have the lowest ranging from [5, 10, 15, 20, 25, 30] points. The invaders shoot at the player as well, which causes the player to lose lives, therefore using reinforcement learning techniques can help find methods to best strategies in the game.\n",
    "\n",
    "\n",
    "Space Invaders has been used frequently as a subject of interest in the field of AI and game theory because of the game's simplicity and complexity at the same time. Heuristic search algorithms such as A-star and genetic algorithms have been used to evaluate each possible move. Reinforcement learning techniques such as Q-learning or Markov Decision Processes have been used to train AI agents to play Space Invaders at a high level of proficiency.\n",
    "\n",
    "\n",
    "Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state. Therefore, deep Q-learning just needs to take in the state, instead of using the state and action, to get the Q-Values. Deep Q-learning benefits using convolutional neural networks, since it helps to get over the temporal limitation of just having one frame, since with multiple frames it is able to see what to shoot at.\n",
    "\n",
    "\n",
    "AI is significant in Space Invaders because it gives a fun and interesting platform to develop and test AI algorithms.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Our problem statement is to create an agent that is able to play Space invaders to that of the same skills set of a human player or better; that is a human player could get a higher score in Space Invaders. Meaning that the agent should be able to obtain a high score by shooting invaders before they reach the bottom of the screen, while also avoiding shots from the invaders. This can be done by allowing the agent to train over 1000 episodes and to learn the best approach for it to get a high score.\n",
    "\n",
    "\n",
    "The state of the game at any given time can be represented as a matrix of pixels, and the actions can be represented as a discrete set of [no action, left, right, fire, fire left, fire right]. The reward function can be defined as the game score, which increases when an invader is destroyed while the invaders in the back rows are worth more points [5, 10, 15, 20, 25, 30] respectively.\n",
    "\n",
    "\n",
    "The performance of the agent can be measured by the game score where a higher score means a better performance. At the end of each game the score can be viewed.\n",
    "\n",
    "\n",
    "The initial state of the game is always the same, with the player’s laser cannon at the bottom of the screen and a formation of invaders at the top with the same positioning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since the agent is being trained with deep q learning, we do not need any data, instead we will use an environment for the agent to learn in:\n",
    "- https://www.gymlibrary.dev/environments/atari/space_invaders/\n",
    "- This environment has an action space of 18 possible actions the agent can take.\n",
    "- The observation space of this environment is 201 x 160, and is colored.\n",
    "- Rewards/points are earned by destroying as many invaders as possible. Invaders destroyed are worth 5, 10, 15, 20, 25, 30 points in the first through sixth rows respectively.\n",
    "- There are an infinite amount of points that can be earned until the agent loses the battle.\n",
    "- A player has a total of 3 lives, which can be lost if the player gets shot by the invaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "For this project, we will use OpenAI's Space Invaders environment from the Atari learning environments. We first will create a Q-Network which is a convolutional neural network that will be used to estimate Q-values. The architecture of our CNN consists of 3 convolutional layers, each followed by a ReLU activation function, and finally we will include 2 fully connected layers. With this completed, we will then move on to create a buffer to store experience tuples to memory such as state, action, reward, next state, and when the agent is done. With this data, we will randomly sample a batch of experiences from memory for training.\n",
    "\n",
    "\n",
    "The DQN agent will then interact with the environment and be trained with the Q-network. The agent will be able to act in the environment, save these new experiences, learn from these new experiences from the training process, and will be able to update the network with new weights. This will all be done over the span of 1000 episodes, with a starting epsilon value of 1.0, an ending epsilon value of .01, and an epsilon decay rate of .99.\n",
    "\n",
    "\n",
    "The agent will learn to play the game by interacting with the environment, obtaining rewards for shooting down invaders, and penalties for getting hit by enemies or letting the invaders reach the bottom of the screen.\n",
    "\n",
    "\n",
    "To visualize this, we will plot the scores obtained by the agent over the 1000 episodes, and we will create a video of the agent playing Space Invaders.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "There are two forms of evaluation that make sense to use when playing Space Invaders: time played and score. Therefore, the evaluation for this agent will be the average score it obtains when playing Space Invaders over a series of game episodes. Another metric can be the average duration of the game over a series of games. These evaluations help determine how well the agent is playing because they provide insights into the average score the agent achieves, which is a good indicator of its ability to accumulate points. The average time metric helps assess how long the agent can last before losing the game. The metric we will mainly be using will be from the average score as that is easier to measure the capabilities of our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "### Subsection 1\n",
    "\n",
    "For this project, we decided to use deep Q-learning, specifically through a deep Q-network (DQN), as our main algorithm rather than just a vanilla Q-learning algorithm. We decided to use this approach because we knew Q-learning could have some drawbacks when dealing with large state spaces as the size of the Q-table grows exponentially with the number of states and actions. This will cause the Q-learning to become computationally expensive not only in memory but also in training time. DQN simply receives the state as an input and outputs the Q-values for all possible actions, hence reducing computational expenses. Because of DQN’s versatility and ability to adapt from large datasets of state-action pairs, we can use DQN to represent the Q-function to solve our problem. The difficulties we may encounter may be due to the computation and finding good hyperparameters as DQN is quite sensitive to them.\n",
    "\n",
    "![QNET](images/QNetwork.png)\n",
    "\n",
    "We use a convolutional neural network built with pytorch to approximate our Q-values for reinforcement learning and CNN to use for feature extractions to determine our best actions. We also then use a Replay buffer to manage our past experiences to help train our model. We use this Replay buffer to randomly sample experiences and extract the rewards and actions etc from each experience and we use these samples to help train our model. Lastly we have a DQN agent which specifically interacts with the environment which helps enforce our current policy and feeds information gained from our policy and environment interactions to our Qnetwork which helps further train our model.\n",
    "\n",
    "\n",
    "### Subsection 2\n",
    "![1000episodes](images/1000episodes.png)\n",
    "\n",
    "Since we see the high volatility, that is the score is ranging between 0 to 800 across 1000 episodes. The computational time for this model to be trained on over 1000 episodes was an estimate of 15 hours on a CPU. One disadvantage of deep Q-learning is that it may take more than 1000s of frames in order for it to fully optimize, but with the computational time we decided to limit the amount of episodes in order for us to lower the amount of time needed to train the agent. Therefore over a long period, we see that deep Q-learning was not able to optimize. We can suggest that the deep Q-learning model is being biased within a specific move that it is capturing within the game. Therefore one method that will help reduce this instability will be to do double deep Q-learning, which can help with overestimating bias. This is done by separating the Q-networks, where one is for target q-values and the other is for current q-values. \n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "DQN uses an epsilon-greedy exploration strategy to balance exploration and exploitation. During training, the agent selects random actions with probability epsilon and selects the action with the highest Q-value with probability (1 - epsilon). For this reason, we decided to be careful with this parameter and opted for the recommended values–ε_start = 1.0, ε_end = .01, and ε_decay = .99. Of course, these values are not fixed, however, in an ideal situation, starting with 1.0 as the starting epsilon value ensures that the agent will explore the environment thoroughly; ending with a low epsilon value ensures that the agent will primarily exploit its learned policy while still exploring enough to refine its policy. Moreover, having a .99 decay rate helps the agent to gradually transition from exploration to exploitation.\n",
    "\n",
    "We can see that the agent seemed to fluctuate between the same values throughout the course of 1000 episodes, having an average reward of 200.\n",
    "\n",
    "Our agent has about 18 possible actions it could take, however, there are many combinations this agent can take. Since we trained the agent for 1k episodes, we consider using a slower decay rate would have provided more exploration and more exploitation at the same time. With this, the agent would have been able to understand the environment more in depth while at the same time, being able to act upon what it has learned. Overall, the agent would have had a higher average reward.\n",
    "\n",
    "![epsilon](images/epsilon.png)\n",
    "\n",
    "As shown in our code for our policy, as episodes increase, the less likely we will choose a random action, meaning we will prioritize exploitation rather than exploration the longer our training happens. A very typical example of an epsilon-greedy policy.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "- Using deep Q-learning, is a better method of using simple Q-learning, yet it has its drawbacks\n",
    "    - Space invaders game has complicated states, which using Q-learning will take a really long time for it to converge, therefore using deep Q-learning will reduce that time.\n",
    "    - It’s able to reduce the problem by not needing to use actions,  since it only needs to take the state to get the q-values.\n",
    "    - With this it becomes unstable, since it is getting the max q-values, causing it to overestimate.\n",
    "    - This can further be optimized by using double deep Q-learning, which will make two Q-networks, where one is for target q-values and the other is for current q-values. \n",
    "    - Our results support this since the agent’s score was volatile, showing instability yet having a time reduction compared to Q-learning since the observation space is 256^100800.\n",
    "- Convolutional neural networks extract features so that the agent may learn\n",
    "    - CNNs are important for agents to learn, since it can help overcome temporal limitations. \n",
    "    - That is the agent will not know which direction the environment is moving unless there  are more frames included so that the agent may learn.\n",
    "    - Therefore the CNN will extract features to determine the best actions based on a current frame.\n",
    "    - We noticed from our mp4 file of the agent playing that there were no projectiles from the invaders, even though the agent was losing lives. \n",
    "    - This may indicate that the agent was not able to learn to move away from the projectiles, therefore losing lives, which can be seen in our mp4s file, where both of the agents had similar movements. \n",
    "\n",
    "\n",
    "### Limitations\n",
    "There are a couple of limitations we can think of while building this project.\n",
    "\n",
    "\n",
    "In a typical and basic deep learning task such as image classification, training a model with a deeper neural network would have produced better results. However, in this case, if a deeper network would have been used, there would have been more instability during training as deep Q-learning tends to be unstable as is.\n",
    "\n",
    "\n",
    "As mentioned before, perhaps if we would have played around with different epsilon hyperparameters, there would have been more promising results. Since we trained the agent over the span of 1000 episodes, one hyperparameter we would test is using a smaller epsilon decay.\n",
    "\n",
    "\n",
    "We trained the agent for 1000 episodes but perhaps if we had trained it longer it would have produced better results. Applying the new epsilon decay hyperparameter and training the agent for a longer amount of time, we believe that the agent would have learned more, hence, being able to play Space Invaders more with precision and accuracy.\n",
    "\n",
    "\n",
    "Another thing we could’ve done is also verify that our parameters are updating correctly and to review our epsilon greedy policy. As mentioned it may be because our model preferred a specific move which caused us to receive suboptimal results. A way we could have improved our model may ha\tve been to ensure our parameters are fed and preprocessed correctly and verify our states are also being preprocessed correctly. We also could have examined our loss values as they give us insight whether or not our policy is effectively working.\n",
    "\n",
    "\n",
    "Overall, we believe that if we were to improve on both of these aspects, there would have been an overall improvement in our results. Training the DQN over the span of a longer time with better hyperparameters would be some aspects we would test further in the future.\n",
    "\n",
    "\n",
    "### Ethics & Privacy\n",
    "While training the agent, there could be issues concerning fairness. The individual may have biases when deciding how to train the agent. For example, there could be bias in determining which actions deserve a reward and which do not. This could lead to an unfair algorithm. Another ethics that we may run into is when we play Space Invaders online. Since our project doesn't require any outside data, but our agent can be used against others, we will not be using this agent against others and only be training our agent within a closed environment. We will be objective by strictly following the rules that have governed Space Invaders for years.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The main goal of this project was to train an agent so that it could optimally play Space Invaders by destroying as many invaders as possible, and therefore earning as many points as possible. We used DQN as the main algorithm, trained the agent for 1000 episodes with an epsilon-greedy exploration strategy. Since our DQN agent seemed to have an average reward of 200, perhaps testing and retraining the agent with different epsilon values over the span of varying episodes, would have produced more promising results.\n",
    "\n",
    "\n",
    "Testing these theories more in depth could serve as an exploration experience for any future research in reinforcement learning. It could serve as a way to further understand deep Q-learning as a whole, hence, being able to build more optimal models in the time to come.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"admonishnote\"></a>1.[^](#pytorch): Reinforcement Learning (DQN) Tutorial *Pytorch* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html<br>\n",
    "<a name=\"huggingface\"></a>2.[^](#huggingfacerl): The Deep Q-Learning Algorithm *Hugging Face*. https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm<br> \n",
    "<a name=\"admonishnote\"></a>3.[^](#medium): Mastering Deep Q-Learning with Pytorch: Comprehensive Guide *Medium* https://medium.com/@hkabhi916/mastering-deep-q-learning-with-pytorch-a-comprehensive-guide-a7e690d644fc<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
